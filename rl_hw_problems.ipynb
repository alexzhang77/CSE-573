{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI's gym is now deprecated, https://gymnasium.farama.org should be used instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0fFgFRGksH6"
   },
   "source": [
    "## Installation\n",
    "\n",
    "To run this you need several packages. First of all, you need anaconda, which you most likely already have if you're viewing this through jupyter. If not then check the readme on the class page.\n",
    "\n",
    "System requirements: This should work on all operating systems (Linux, Mac, and Windows). However, several of the environments in the OpenAI-gym require additional simulators which don't aren't easy to get on Windows. In any case, it is strongly recommended that you use Linux, although you should be ok with Mac. (HINT: if you're on Windows check out the Windows Subsystem for Linux (WSL), although it'll make visualizing your policies a little tricky).\n",
    "\n",
    "Then install the following packages (using conda or pip):\n",
    "\n",
    "- pytorch --> `conda install pytorch -c pytorch`\n",
    "- gym --> `pip install gym`\n",
    "- gym (the cool environments, doesnt work on Windows) --> `pip install gym[all]`\n",
    "(When install gym[all] don't worry if the mujoco installation doesn't work. That's a more advanced 3D physics simulator that has to be set up separately (see website). Anyway, we don't need it necessarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nomKMuz0lrWL"
   },
   "outputs": [],
   "source": [
    "# If you're using colab, this will install the necessary packages!\n",
    "!pip install torch\n",
    "!pip install gymnasium\n",
    "!pip install numpy==1.24.1\n",
    "!pip install \"gymnasium[classic-control]\"\n",
    "!pip install swig\n",
    "!pip install \"gymnasium[box2d]\"\n",
    "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "!pip install shimmy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzmPUAD1ksH7"
   },
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.manual_seed(573)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch import distributions\n",
    "from torch.distributions import Categorical\n",
    "from itertools import islice\n",
    "import ale_py\n",
    "import shimmy\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXry0cZLksH_"
   },
   "source": [
    "# Introduction\n",
    "Welcome to the RL playground. Your task is to implement the REINFORCE and A3C algorithm to solve various OpenAI-gym environments. If you are not familiar with OpenAI-gym, stop reading and visit https://gym.openai.com/envs/ to see all the tasks you can try to solve.\n",
    "\n",
    "In this homework, we will only look at tasks with a discrete (and small) action space. That being said, both algorithms can be modified slightly to work on tasks with continuous action spaces. For full credit you must fill in the code below so you achieve an average total reward per episode on the cartpole task (CartPole-v1) of at least 499 (for an episode length of 500) for both REINFORCE and A3C. Then you must apply your code to any one other environment in OpenAI-gym, and plot and compare the learning curves (average total reward per episode vs number of episodes trained on) between REINFORCE and A3C (where at least one of the algorithms shows significant improvement from initialization).\n",
    "\n",
    "Below there's an overview of what every iteration will look like, regardless of whether you want to train or evaluate your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBY204WCksIA"
   },
   "outputs": [],
   "source": [
    "from rlhw_util import * # <-- look whats inside here - it could save you a lot of work!\n",
    "\n",
    "def run_iteration(mode, N, agent, gen, horizon=None, render=False):\n",
    "    train = mode == 'train'\n",
    "    if train:\n",
    "        agent.train()\n",
    "    else:\n",
    "        agent.eval()\n",
    "    states, actions, rewards, prev_states = zip(*[gen(horizon=horizon, render=render) for _ in range(N)])\n",
    "    loss = None\n",
    "    if train:\n",
    "        loss = agent.learn(states, actions, rewards, prev_states)\n",
    "\n",
    "    reward = sum([r.sum() for r in rewards]) / N\n",
    "\n",
    "    return reward, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FbOI_tXDksIC"
   },
   "source": [
    "## The Actor\n",
    "\n",
    "We need to learn a policy which, given some state, outputs a distribution over all possible actions. As this is deep RL, we'll use a deep neural network to turn the observed state into the requisite action distribution. From this action distribution we can choose what action to take using `get_action`. Pytorch, brilliant as it is, makes our task incredibly easy, as we can use the `torch.distributions.Categorical` class for sampling.\n",
    "\n",
    "You can experiment with all sorts of network architectures, but remember this is RL, not image classification on ImageNet, so you probably won't need a very deep network (HINT: look below at the state and action dimensionality to get a feel for the task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkW0tUvZksID"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \n",
    "        x = torch.relu(self.fc1(state))\n",
    "        return torch.softmax(self.fc2(x), dim=-1)\n",
    "\n",
    "    def get_policy(self, state):\n",
    "        return Categorical(self(state))\n",
    "\n",
    "    def get_action(self, state, greedy=None):\n",
    "        if greedy is None:\n",
    "            greedy = not self.training\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        return MLE(policy) if greedy else policy.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NYbREzFksIF"
   },
   "source": [
    "## The REINFORCE Agent\n",
    "\n",
    "The Actor defines our policy, but we also have to define how and when we'll be updating our policy, which brings us to the agent. The agent will house the policy (an `Actor`), and can then be used to generate rollouts (using `forward()`) or update the policy given a list of rollouts (using `learn()`).\n",
    "\n",
    "The REINFORCE algorithm naively uses the returns directly to weight the gradients, however this makes the variance in the policy gradient estimation very large. As a result, we will use a baseline which is a linear model which takes in a state and outputs the return (sounds like a value function, right?). Except we're not going to train our baseline using gradient descent, instead we'll just solve the linear system analytically in every iteration, and use the solution in the next iteration. Don't worry about training/updating the baseline, but you do have to use it in the right way. (Optional experiment: try removing the baseline and see how performance changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQ7mlwvLksIG"
   },
   "outputs": [],
   "source": [
    "class REINFORCE(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4):\n",
    "        super(REINFORCE, self).__init__()\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        \n",
    "        self.baseline = nn.Linear(state_dim, 1)\n",
    "        \n",
    "        self.optimizer = optim.RMSprop(self.actor.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        self.discount = discount\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actor.get_action(state)\n",
    "    \n",
    "    def learn(self, states, actions, rewards, _):\n",
    "        '''\n",
    "        Takes in three arguments each of which is a list with equal length. Each element in the list is a \n",
    "        pytorch tensor with 1 row for every step in the episode, and the columns are state_dim, action_dim, \n",
    "        and 1, respectively.\n",
    "        '''\n",
    "        \n",
    "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
    "        \n",
    "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
    "        \n",
    "        # Compute advantages (returns - baseline predictions)\n",
    "        with torch.no_grad():\n",
    "            baseline_values = self.baseline(states).squeeze()\n",
    "            advantages = returns - baseline_values\n",
    "\n",
    "        # Compute the log probabilities of the actions taken\n",
    "        log_probs = []\n",
    "        for state, action in zip(states, actions):\n",
    "            policy = self.actor.get_policy(state)\n",
    "            log_probs.append(policy.log_prob(action).unsqueeze(0))\n",
    "        log_probs = torch.cat(log_probs, dim=0)\n",
    "\n",
    "        # Compute the policy loss (negative of the weighted log-probabilities)\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "\n",
    "        # Update the actor's parameters\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        error = F.mse_loss(self.baseline(states).squeeze(), returns).detach()\n",
    "        solve(states, returns, out=self.baseline)\n",
    "        #error = F.mse_loss(self.baseline(states).squeeze(), returns).detach()\n",
    "        \n",
    "        return error.item() # Returns a rough estimate of the error in the baseline (dont worry about this too much)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ogkj6xjeksII"
   },
   "source": [
    "## The Critic\n",
    "\n",
    "Now we can introduce a critic, which is essentially a value function to estimate the expected discounted reward of a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uGYwOY6AksIJ"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_dim, 64)  # First fully connected layer\n",
    "        self.fc2 = nn.Linear(64, 1)  # Output layer to estimate value\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        x = torch.relu(self.fc1(state))  # Apply ReLU to the first layer\n",
    "        value = self.fc2(x)  # Get the state value\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rfkylePksIL"
   },
   "source": [
    "## The A3C Agent\n",
    "\n",
    "Now we can put the actor and critic together using the A3C algorithm. It turns out, the tasks in the gym are all so simple that there is essentially no gain in parallelization, so technically we're implementing A2C (no async), but the RL part is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QoZgaABTksIL"
   },
   "outputs": [],
   "source": [
    "class A3C(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4):\n",
    "        super(A3C, self).__init__()\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.RMSprop(self.actor.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.critic_optimizer = torch.optim.RMSprop(self.critic.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        self.discount = discount\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actor.get_action(state)\n",
    "    \n",
    "    def learn(self, states, actions, rewards, _):\n",
    "        \n",
    "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
    "        \n",
    "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
    "        \n",
    "        values = self.critic(states).squeeze()\n",
    "        advantages = returns - values.detach() \n",
    "\n",
    "        # Actor loss (policy gradient with advantages)\n",
    "        log_probs = []\n",
    "        for state, action in zip(states, actions):\n",
    "            policy = self.actor.get_policy(state)\n",
    "            log_probs.append(policy.log_prob(action).unsqueeze(0))\n",
    "        log_probs = torch.cat(log_probs, dim=0)\n",
    "        actor_loss = -(log_probs * advantages).mean()\n",
    "\n",
    "        # Critic loss (value function regression)\n",
    "        critic_loss = F.mse_loss(values, returns)\n",
    "\n",
    "        # Update actor network\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update critic network\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4, clip_epsilon=0.2, \n",
    "                 actor_epochs=10, critic_epochs=10, max_grad_norm=0.5):\n",
    "        super(PPO, self).__init__()\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.RMSprop(self.actor.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.critic_optimizer = torch.optim.RMSprop(self.critic.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        self.discount = discount\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.actor_epochs = actor_epochs\n",
    "        self.critic_epochs = critic_epochs\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.actor.get_action(state)\n",
    "\n",
    "    def get_action(self, state, greedy=None):\n",
    "        if greedy is None:\n",
    "            greedy = not self.training\n",
    "\n",
    "        policy = self.actor.get_policy(state)\n",
    "        return MLE(policy) if greedy else policy.sample()\n",
    "\n",
    "    def learn(self, states, actions, rewards, _):\n",
    "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
    "\n",
    "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
    "        advantages = returns - self.critic(states).squeeze().detach()\n",
    "\n",
    "        old_policies = []\n",
    "        for state, action in zip(states, actions):\n",
    "            policy = self.actor.get_policy(state)\n",
    "            old_policies.append(policy.log_prob(action).unsqueeze(0))\n",
    "        old_policies = torch.cat(old_policies, dim=0).detach()\n",
    "\n",
    "        for _ in range(self.actor_epochs):\n",
    "            log_probs = []\n",
    "            for state, action in zip(states, actions):\n",
    "                policy = self.actor.get_policy(state)\n",
    "                log_probs.append(policy.log_prob(action).unsqueeze(0))\n",
    "            log_probs = torch.cat(log_probs, dim=0)\n",
    "\n",
    "            ratios = torch.exp(log_probs - old_policies)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "        for _ in range(self.critic_epochs):\n",
    "            values = self.critic(states).squeeze()\n",
    "            critic_loss = F.mse_loss(values, returns)\n",
    "\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from torch import tensor\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state):\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        return torch.cat(states), torch.cat(actions), torch.cat(rewards), torch.cat(next_states)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, weight_decay=1e-4, gamma=0.99, epsilon=0.9, epsilon_decay=0.999, min_epsilon=0.05, buffer_capacity=10000, batch_size=128, tau = 0.005):\n",
    "        super(DQN, self).__init__()\n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "\n",
    "        self.target_q_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.AdamW(self.q_network.parameters(), amsgrad=True)\n",
    "\n",
    "        # Set hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "\n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "\n",
    "        # Copy initial weights to the target network\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Given a state, predict Q-values for all actions.\"\"\"\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "        rndsmp = torch.rand(1).item()\n",
    "        if rndsmp > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "                action = tensor([q_values.argmax()])\n",
    "        else:\n",
    "            return torch.randint(0, self.q_network[-1].out_features, (1,))\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def get_action(self, state, greedy=None):\n",
    "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n",
    "        rndsmp = torch.rand(1).item()\n",
    "        if rndsmp > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "                action = q_values.argmax().item()\n",
    "        else:\n",
    "            q_values = self.q_network(state)\n",
    "            action = q_values.argmax().item()\n",
    "            action = torch.randint(0, self.q_network[-1].out_features, (1,)).item()\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def learn(self, states, actions, rewards, prev_states): # we will just sample states, actions, and rewards, but we need to appease the syntax\n",
    "        \"\"\"Update the Q-network using the DQN loss.\"\"\"\n",
    "        states, actions, rewards, prev_states = torch.cat(states), torch.cat(actions), torch.cat(rewards), torch.cat(prev_states)\n",
    "        self.replay_buffer.add(prev_states, actions, rewards, states)\n",
    "        if self.replay_buffer.size() < self.batch_size:\n",
    "            return None\n",
    "        # Sample from replay buffer\n",
    "        states, actions, rewards, next_states = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        \n",
    "        # Compute target Q-values for next states\n",
    "        with torch.no_grad():\n",
    "            max_next_q_values = self.target_q_network(next_states).max(dim=1).values\n",
    "            targets = rewards + self.gamma * max_next_q_values\n",
    "\n",
    "        # Compute Q-values for current states\n",
    "        q_values = self.q_network(states).gather(1, actions)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.mse_loss(q_values, targets.unsqueeze(1))\n",
    "\n",
    "        # Update the Q-network\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        target_net_state_dict = self.target_q_network.state_dict()\n",
    "        policy_net_state_dict = self.q_network.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + target_net_state_dict[key]*(1-self.tau)\n",
    "        self.target_q_network.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hq-zQWzAksIO"
   },
   "source": [
    "## Part 1: Balancing a pole with a cart\n",
    "\n",
    "First, we'll test both algorithms on a very simple toy system: the cartpole. Eventhough it's very low dimensional (state=4, action=2), this task is nontrival because it is underactuated. Nevertheless after a few thousand episodes our policy shouldn't have a problem! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RGBdP7leksIP"
   },
   "outputs": [],
   "source": [
    "# Optimization hyperparameters\n",
    "lr = 0.005\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8vAi8sXuksIS"
   },
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v3\" \n",
    "#env_name = 'MountainCar-v0'\n",
    "e = Pytorch_Gym_Env(env_name)\n",
    "state_dim = e.observation_space.shape[0]\n",
    "action_dim = e.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Meq1LT8NksIV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Choose what agent to use\n",
    "agent = PPO(state_dim, action_dim, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "total_episodes = 0\n",
    "print(agent) # Let's take a look at what we're working with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUl_VOypksIX"
   },
   "outputs": [],
   "source": [
    "# Create a \n",
    "gen = Generator(e, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ja001im9ksIZ"
   },
   "source": [
    "## Let's do this!!\n",
    "\n",
    "Below is the loop to train and evaluate your agent. You can play around with the number of iterations to run, and the number of rollouts per iteration. \n",
    "\n",
    "You can rerun this cell multiple times to keep training your model for more episodes. In any case, it shouldn't take more than 30 min to an 1 hour to train. (training never took me more than 5 min). HINT: Keep an eye on the eval_reward, it'll be pretty noisy, but if that should be slowly increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8oT0vzayksIa"
   },
   "outputs": [],
   "source": [
    "num_iter = 5000\n",
    "num_train = 10\n",
    "num_eval = 10 # dont change this\n",
    "rewards_list = []\n",
    "episodes_list = []\n",
    "for itr in range(num_iter):\n",
    "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
    "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
    "    train_reward, train_loss = run_iteration('train', num_train, agent, gen)\n",
    "    eval_reward, _ = run_iteration('eval', num_eval, agent, gen)\n",
    "    total_episodes += num_train\n",
    "    print('Ep:{}:'.format(total_episodes))\n",
    "    print('reward={:.3f}'.format(train_reward))\n",
    "    print(f'train loss: {train_loss}')\n",
    "    print('eval={:.3f}'.format(eval_reward))\n",
    "    rewards_list.append(eval_reward)\n",
    "    episodes_list.append(total_episodes)\n",
    "    #CartPole-v1 LunarLander-v3\n",
    "    if eval_reward > 199 and env_name == 'LunarLander-v3': # dont change this\n",
    "        print('Success!!! You have solved cartpole task! Time for a bigger challenge!')\n",
    "        break\n",
    "    \n",
    "    # save model\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxlxf0YwksIc"
   },
   "outputs": [],
   "source": [
    "# You can visualize your policy at any time\n",
    "vis_gen = Generator(Pytorch_Gym_Env(env_name, render_mode=\"human\"), agent)\n",
    "run_iteration('eval', 1, agent, vis_gen, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(A3C_episodes_list, A3C_rewards_list, label=\"A3C\")\n",
    "plt.plot(REINFORCE_episodes_list, REINFORCE_rewards_list, label=\"REINFORCE\")\n",
    "plt.plot(DQN_episodes_list, DQN_rewards_list, label=\"DQN\")\n",
    "plt.plot(PPO_episodes_list, PPO_rewards_list, label=\"PPO\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Evaluation Rewards')\n",
    "plt.title(\"Performances on CartPole\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(PPO_rewards_list))\n",
    "print(max(A3C_rewards_list))\n",
    "print(max(REINFORCE_rewards_list))\n",
    "print(max(DQN_rewards_list))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "rl-hw-problems.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "RL_HW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
